{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to the dataframe\n"
     ]
    }
   ],
   "source": [
    "f = open('nlp_train.json') \n",
    "import pandas as pd\n",
    "import json\n",
    "data = json.load(f) \n",
    "\n",
    "    # Iterating through the json \n",
    "    # list \n",
    "    #print(data[\"fkrr36o\"][\"emotion\"])\n",
    "f.close() \n",
    "fl=[]\n",
    "    #print(data[\"fkrr36o\"])\n",
    "for u,v in data.items():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    l.append(v[\"body\"])\n",
    "    l.append(1 if v[\"emotion\"][\"anger\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"anticipation\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"disgust\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"fear\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"joy\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"love\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"optimism\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"pessimism\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"sadness\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"surprise\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"trust\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"neutral\"]==True else 0)\n",
    "    fl.append(l)\n",
    "\n",
    "df = pd.DataFrame(fl, columns=[\"id\",\"body\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"])\n",
    "df.to_pickle(\"./dataframe.pkl\")\n",
    "print(\"Data loaded to the dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataframe.pkl\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    data = data.lower()\n",
    "    data = re.sub('<[^<]+?>', '', data)\n",
    "    data=re.sub('[!#?,.:\";]', '', data)\n",
    "    data=re.sub(r'[0-9]+', '', data)\n",
    "    data= re.sub(\"i'm\",\"i am\",data)\n",
    "    data = stemSentence(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing started\n",
      "Preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing started\")\n",
    "df['body'] = df['body'].map(lambda x : clean(x))\n",
    "print(\"Preprocessing completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 14)\n",
      "(1119, 12)\n",
      "(1119, 12)\n",
      "(1119, 12)\n",
      "(1119, 12)\n",
      "(374,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "emotions = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"]\n",
    "train, test= train_test_split(df, random_state=40, test_size=0.25, shuffle=True)\n",
    "\n",
    "X_train= train['body']\n",
    "X_test= test['body']\n",
    "y_train=train.iloc[:,2:]\n",
    "y_test=test.iloc[:,2:]\n",
    "print(train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(y1_train):\n",
    "    Y1_train=[]\n",
    "    \n",
    "    for i in y1_train:\n",
    "        if i==0:\n",
    "            Y1_train.append([1,0])\n",
    "        else:\n",
    "            Y1_train.append([0,1])\n",
    "    return Y1_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train = prep(train[\"anger\"].values)\n",
    "y1_test =  prep(train[\"anger\"].values)\n",
    "\n",
    "\n",
    "y2_train = prep(train[\"anticipation\"].values)\n",
    "y2_test =  prep(train[\"anticipation\"].values)\n",
    "\n",
    "y3_train = prep(train[\"disgust\"].values)\n",
    "y3_test =  prep(train[\"disgust\"].values)\n",
    "\n",
    "y4_train = prep(train[\"fear\"].values)\n",
    "y4_test =  prep(train[\"fear\"].values)\n",
    "\n",
    "y5_train = prep(train[\"joy\"].values)\n",
    "y5_test =  prep(train[[\"joy\"]].values)\n",
    "\n",
    "y6_train = prep(train[\"love\"].values)\n",
    "y6_test =  prep(train[\"love\"].values)\n",
    "\n",
    "y7_train = prep(train[\"optimism\"].values)\n",
    "y7_test =  prep(train[\"optimism\"].values)\n",
    "\n",
    "y8_train = prep(train[\"pessimism\"].values)\n",
    "y8_test =  prep(train[\"pessimism\"].values)\n",
    "\n",
    "y9_train = prep(train[\"sadness\"].values)\n",
    "y9_test =  prep(train[\"sadness\"].values)\n",
    "\n",
    "y10_train = prep(train[\"surprise\"].values)\n",
    "y10_test =  prep(train[\"surprise\"].values)\n",
    "\n",
    "y11_train = prep(train[\"trust\"].values)\n",
    "y11_test =  prep(train[\"trust\"].values)\n",
    "\n",
    "y12_train = prep(train[\"neutral\"].values)\n",
    "y12_test =  prep(train[\"neutral\"].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "embeddings_dictionary={}\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\n",
    "LSTM_Layer1 = LSTM(128)(embedding_layer)\n",
    "#LSTM_Layer2 = LSTM(128)(LSTM_Layer1)\n",
    "\n",
    "output1 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output2 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output3 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output4 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output5 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output6 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output7 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output8= Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output9 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output10 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output11 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output12 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6, output7, output8,output9, output10, output11, output12])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 100)     1980600     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 128)          117248      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            258         lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,100,944\n",
      "Trainable params: 120,344\n",
      "Non-trainable params: 1,980,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 200)\n",
      "(1119, 12)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 895 samples, validate on 224 samples\n",
      "Epoch 1/30\n",
      "895/895 [==============================] - 11s 12ms/step - loss: 7.9649 - dense_14_loss: 0.7040 - dense_15_loss: 0.6912 - dense_16_loss: 0.6948 - dense_17_loss: 0.6968 - dense_18_loss: 0.6076 - dense_19_loss: 0.6533 - dense_20_loss: 0.6705 - dense_21_loss: 0.6932 - dense_22_loss: 0.6891 - dense_23_loss: 0.5973 - dense_24_loss: 0.6272 - dense_25_loss: 0.6399 - dense_14_f1: 0.5468 - dense_15_f1: 0.5669 - dense_16_f1: 0.5149 - dense_17_f1: 0.6125 - dense_18_f1: 0.8099 - dense_19_f1: 0.6710 - dense_20_f1: 0.6808 - dense_21_f1: 0.4408 - dense_22_f1: 0.6739 - dense_23_f1: 0.8186 - dense_24_f1: 0.7009 - dense_25_f1: 0.7621 - val_loss: 7.3134 - val_dense_14_loss: 0.6760 - val_dense_15_loss: 0.7044 - val_dense_16_loss: 0.6935 - val_dense_17_loss: 0.6918 - val_dense_18_loss: 0.4686 - val_dense_19_loss: 0.5278 - val_dense_20_loss: 0.6662 - val_dense_21_loss: 0.7033 - val_dense_22_loss: 0.6554 - val_dense_23_loss: 0.4742 - val_dense_24_loss: 0.5473 - val_dense_25_loss: 0.5047 - val_dense_14_f1: 0.6318 - val_dense_15_f1: 0.5806 - val_dense_16_f1: 0.6303 - val_dense_17_f1: 0.6700 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.3108 - val_dense_22_f1: 0.7140 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 2/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 7.0410 - dense_14_loss: 0.6684 - dense_15_loss: 0.6919 - dense_16_loss: 0.6996 - dense_17_loss: 0.6933 - dense_18_loss: 0.4563 - dense_19_loss: 0.4362 - dense_20_loss: 0.6444 - dense_21_loss: 0.6906 - dense_22_loss: 0.6477 - dense_23_loss: 0.4450 - dense_24_loss: 0.5369 - dense_25_loss: 0.4307 - dense_14_f1: 0.6264 - dense_15_f1: 0.5242 - dense_16_f1: 0.5596 - dense_17_f1: 0.6572 - dense_18_f1: 0.8704 - dense_19_f1: 0.9078 - dense_20_f1: 0.6994 - dense_21_f1: 0.4095 - dense_22_f1: 0.7205 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.4291 - val_dense_14_loss: 0.6502 - val_dense_15_loss: 0.7006 - val_dense_16_loss: 0.6879 - val_dense_17_loss: 0.6993 - val_dense_18_loss: 0.2932 - val_dense_19_loss: 0.3398 - val_dense_20_loss: 0.6631 - val_dense_21_loss: 0.6881 - val_dense_22_loss: 0.5752 - val_dense_23_loss: 0.3124 - val_dense_24_loss: 0.4307 - val_dense_25_loss: 0.3886 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.5055 - val_dense_16_f1: 0.5674 - val_dense_17_f1: 0.5490 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5494 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 3/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.4934 - dense_14_loss: 0.6592 - dense_15_loss: 0.6881 - dense_16_loss: 0.6915 - dense_17_loss: 0.6904 - dense_18_loss: 0.3904 - dense_19_loss: 0.3102 - dense_20_loss: 0.6134 - dense_21_loss: 0.6815 - dense_22_loss: 0.6032 - dense_23_loss: 0.3588 - dense_24_loss: 0.4681 - dense_25_loss: 0.3386 - dense_14_f1: 0.6268 - dense_15_f1: 0.5798 - dense_16_f1: 0.4209 - dense_17_f1: 0.5350 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.5825 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3632 - val_dense_14_loss: 0.6511 - val_dense_15_loss: 0.6940 - val_dense_16_loss: 0.7035 - val_dense_17_loss: 0.6835 - val_dense_18_loss: 0.2784 - val_dense_19_loss: 0.3309 - val_dense_20_loss: 0.6469 - val_dense_21_loss: 0.6905 - val_dense_22_loss: 0.5613 - val_dense_23_loss: 0.3156 - val_dense_24_loss: 0.4202 - val_dense_25_loss: 0.3872 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.4682 - val_dense_16_f1: 0.3209 - val_dense_17_f1: 0.5388 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5390 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 4/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.4516 - dense_14_loss: 0.6583 - dense_15_loss: 0.6859 - dense_16_loss: 0.6899 - dense_17_loss: 0.6870 - dense_18_loss: 0.3863 - dense_19_loss: 0.3085 - dense_20_loss: 0.6103 - dense_21_loss: 0.6753 - dense_22_loss: 0.6001 - dense_23_loss: 0.3550 - dense_24_loss: 0.4648 - dense_25_loss: 0.3301 - dense_14_f1: 0.6268 - dense_15_f1: 0.4896 - dense_16_f1: 0.3992 - dense_17_f1: 0.5392 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.5696 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3465 - val_dense_14_loss: 0.6516 - val_dense_15_loss: 0.6927 - val_dense_16_loss: 0.6882 - val_dense_17_loss: 0.6793 - val_dense_18_loss: 0.2842 - val_dense_19_loss: 0.3290 - val_dense_20_loss: 0.6513 - val_dense_21_loss: 0.6899 - val_dense_22_loss: 0.5624 - val_dense_23_loss: 0.3116 - val_dense_24_loss: 0.4231 - val_dense_25_loss: 0.3833 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.4545 - val_dense_16_f1: 0.5567 - val_dense_17_f1: 0.5714 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5302 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 5/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.3992 - dense_14_loss: 0.6563 - dense_15_loss: 0.6791 - dense_16_loss: 0.6867 - dense_17_loss: 0.6789 - dense_18_loss: 0.3846 - dense_19_loss: 0.3056 - dense_20_loss: 0.6063 - dense_21_loss: 0.6706 - dense_22_loss: 0.5940 - dense_23_loss: 0.3471 - dense_24_loss: 0.4623 - dense_25_loss: 0.3276 - dense_14_f1: 0.6268 - dense_15_f1: 0.5654 - dense_16_f1: 0.5552 - dense_17_f1: 0.5481 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.5642 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3273 - val_dense_14_loss: 0.6458 - val_dense_15_loss: 0.6935 - val_dense_16_loss: 0.6861 - val_dense_17_loss: 0.6794 - val_dense_18_loss: 0.2904 - val_dense_19_loss: 0.3244 - val_dense_20_loss: 0.6464 - val_dense_21_loss: 0.6870 - val_dense_22_loss: 0.5567 - val_dense_23_loss: 0.3161 - val_dense_24_loss: 0.4242 - val_dense_25_loss: 0.3773 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.5314 - val_dense_16_f1: 0.5899 - val_dense_17_f1: 0.5761 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5000 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 6/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.3734 - dense_14_loss: 0.6532 - dense_15_loss: 0.6774 - dense_16_loss: 0.6839 - dense_17_loss: 0.6735 - dense_18_loss: 0.3843 - dense_19_loss: 0.3039 - dense_20_loss: 0.6056 - dense_21_loss: 0.6648 - dense_22_loss: 0.5891 - dense_23_loss: 0.3507 - dense_24_loss: 0.4616 - dense_25_loss: 0.3254 - dense_14_f1: 0.6268 - dense_15_f1: 0.6072 - dense_16_f1: 0.5902 - dense_17_f1: 0.5943 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6064 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3082 - val_dense_14_loss: 0.6428 - val_dense_15_loss: 0.6926 - val_dense_16_loss: 0.6860 - val_dense_17_loss: 0.6763 - val_dense_18_loss: 0.2935 - val_dense_19_loss: 0.3230 - val_dense_20_loss: 0.6450 - val_dense_21_loss: 0.6848 - val_dense_22_loss: 0.5513 - val_dense_23_loss: 0.3189 - val_dense_24_loss: 0.4198 - val_dense_25_loss: 0.3742 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.5342 - val_dense_16_f1: 0.5556 - val_dense_17_f1: 0.5709 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5554 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 7/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.3320 - dense_14_loss: 0.6509 - dense_15_loss: 0.6731 - dense_16_loss: 0.6805 - dense_17_loss: 0.6644 - dense_18_loss: 0.3832 - dense_19_loss: 0.3054 - dense_20_loss: 0.6024 - dense_21_loss: 0.6608 - dense_22_loss: 0.5825 - dense_23_loss: 0.3480 - dense_24_loss: 0.4594 - dense_25_loss: 0.3216 - dense_14_f1: 0.6290 - dense_15_f1: 0.6143 - dense_16_f1: 0.5722 - dense_17_f1: 0.6251 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6384 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.2713 - val_dense_14_loss: 0.6349 - val_dense_15_loss: 0.6933 - val_dense_16_loss: 0.6871 - val_dense_17_loss: 0.6674 - val_dense_18_loss: 0.2911 - val_dense_19_loss: 0.3224 - val_dense_20_loss: 0.6463 - val_dense_21_loss: 0.6841 - val_dense_22_loss: 0.5418 - val_dense_23_loss: 0.3143 - val_dense_24_loss: 0.4165 - val_dense_25_loss: 0.3722 - val_dense_14_f1: 0.6442 - val_dense_15_f1: 0.5347 - val_dense_16_f1: 0.5333 - val_dense_17_f1: 0.5912 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5730 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 8/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.2751 - dense_14_loss: 0.6389 - dense_15_loss: 0.6681 - dense_16_loss: 0.6725 - dense_17_loss: 0.6593 - dense_18_loss: 0.3807 - dense_19_loss: 0.3025 - dense_20_loss: 0.5971 - dense_21_loss: 0.6581 - dense_22_loss: 0.5779 - dense_23_loss: 0.3471 - dense_24_loss: 0.4577 - dense_25_loss: 0.3152 - dense_14_f1: 0.6292 - dense_15_f1: 0.6092 - dense_16_f1: 0.5950 - dense_17_f1: 0.5897 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6322 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.2016 - val_dense_14_loss: 0.6273 - val_dense_15_loss: 0.6942 - val_dense_16_loss: 0.6694 - val_dense_17_loss: 0.6618 - val_dense_18_loss: 0.2881 - val_dense_19_loss: 0.3215 - val_dense_20_loss: 0.6425 - val_dense_21_loss: 0.6751 - val_dense_22_loss: 0.5312 - val_dense_23_loss: 0.3121 - val_dense_24_loss: 0.4107 - val_dense_25_loss: 0.3676 - val_dense_14_f1: 0.6442 - val_dense_15_f1: 0.5196 - val_dense_16_f1: 0.6134 - val_dense_17_f1: 0.5660 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.6021 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 9/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.2366 - dense_14_loss: 0.6393 - dense_15_loss: 0.6622 - dense_16_loss: 0.6683 - dense_17_loss: 0.6501 - dense_18_loss: 0.3774 - dense_19_loss: 0.3007 - dense_20_loss: 0.5938 - dense_21_loss: 0.6572 - dense_22_loss: 0.5772 - dense_23_loss: 0.3461 - dense_24_loss: 0.4550 - dense_25_loss: 0.3093 - dense_14_f1: 0.6321 - dense_15_f1: 0.5979 - dense_16_f1: 0.6173 - dense_17_f1: 0.5885 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6164 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1830 - val_dense_14_loss: 0.6145 - val_dense_15_loss: 0.6951 - val_dense_16_loss: 0.6677 - val_dense_17_loss: 0.6751 - val_dense_18_loss: 0.2788 - val_dense_19_loss: 0.3207 - val_dense_20_loss: 0.6425 - val_dense_21_loss: 0.6735 - val_dense_22_loss: 0.5279 - val_dense_23_loss: 0.3146 - val_dense_24_loss: 0.4077 - val_dense_25_loss: 0.3649 - val_dense_14_f1: 0.6493 - val_dense_15_f1: 0.5125 - val_dense_16_f1: 0.5824 - val_dense_17_f1: 0.5904 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.6085 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 10/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.2035 - dense_14_loss: 0.6359 - dense_15_loss: 0.6594 - dense_16_loss: 0.6672 - dense_17_loss: 0.6484 - dense_18_loss: 0.3779 - dense_19_loss: 0.2981 - dense_20_loss: 0.5914 - dense_21_loss: 0.6503 - dense_22_loss: 0.5703 - dense_23_loss: 0.3413 - dense_24_loss: 0.4586 - dense_25_loss: 0.3047 - dense_14_f1: 0.6494 - dense_15_f1: 0.6169 - dense_16_f1: 0.5825 - dense_17_f1: 0.6237 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.7002 - dense_21_f1: 0.6294 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1720 - val_dense_14_loss: 0.6118 - val_dense_15_loss: 0.6984 - val_dense_16_loss: 0.6545 - val_dense_17_loss: 0.6743 - val_dense_18_loss: 0.2749 - val_dense_19_loss: 0.3187 - val_dense_20_loss: 0.6545 - val_dense_21_loss: 0.6784 - val_dense_22_loss: 0.5224 - val_dense_23_loss: 0.3142 - val_dense_24_loss: 0.4116 - val_dense_25_loss: 0.3585 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.5666 - val_dense_16_f1: 0.5836 - val_dense_17_f1: 0.5797 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5645 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 11/30\n",
      "895/895 [==============================] - 9s 10ms/step - loss: 6.1541 - dense_14_loss: 0.6231 - dense_15_loss: 0.6609 - dense_16_loss: 0.6537 - dense_17_loss: 0.6516 - dense_18_loss: 0.3743 - dense_19_loss: 0.2953 - dense_20_loss: 0.5892 - dense_21_loss: 0.6457 - dense_22_loss: 0.5680 - dense_23_loss: 0.3382 - dense_24_loss: 0.4551 - dense_25_loss: 0.2991 - dense_14_f1: 0.6487 - dense_15_f1: 0.6215 - dense_16_f1: 0.6072 - dense_17_f1: 0.6242 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.7004 - dense_21_f1: 0.6321 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.2825 - val_dense_14_loss: 0.6353 - val_dense_15_loss: 0.6982 - val_dense_16_loss: 0.6914 - val_dense_17_loss: 0.6728 - val_dense_18_loss: 0.2976 - val_dense_19_loss: 0.3187 - val_dense_20_loss: 0.6371 - val_dense_21_loss: 0.6741 - val_dense_22_loss: 0.5485 - val_dense_23_loss: 0.3195 - val_dense_24_loss: 0.4178 - val_dense_25_loss: 0.3715 - val_dense_14_f1: 0.6080 - val_dense_15_f1: 0.5583 - val_dense_16_f1: 0.5786 - val_dense_17_f1: 0.5817 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5993 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 12/30\n",
      "895/895 [==============================] - 10s 11ms/step - loss: 6.3413 - dense_14_loss: 0.6617 - dense_15_loss: 0.6601 - dense_16_loss: 0.7050 - dense_17_loss: 0.6607 - dense_18_loss: 0.3783 - dense_19_loss: 0.3008 - dense_20_loss: 0.5922 - dense_21_loss: 0.6826 - dense_22_loss: 0.5917 - dense_23_loss: 0.3379 - dense_24_loss: 0.4517 - dense_25_loss: 0.3184 - dense_14_f1: 0.6561 - dense_15_f1: 0.6215 - dense_16_f1: 0.5495 - dense_17_f1: 0.6030 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.7006 - dense_21_f1: 0.5850 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.6018 - val_dense_14_loss: 0.7057 - val_dense_15_loss: 0.7113 - val_dense_16_loss: 0.7526 - val_dense_17_loss: 0.6796 - val_dense_18_loss: 0.2733 - val_dense_19_loss: 0.3374 - val_dense_20_loss: 0.6858 - val_dense_21_loss: 0.7863 - val_dense_22_loss: 0.5554 - val_dense_23_loss: 0.3129 - val_dense_24_loss: 0.4158 - val_dense_25_loss: 0.3856 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.6008 - val_dense_16_f1: 0.5670 - val_dense_17_f1: 0.5423 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5402 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 13/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.4779 - dense_14_loss: 0.6732 - dense_15_loss: 0.6765 - dense_16_loss: 0.7187 - dense_17_loss: 0.6694 - dense_18_loss: 0.3902 - dense_19_loss: 0.3046 - dense_20_loss: 0.6108 - dense_21_loss: 0.7001 - dense_22_loss: 0.5971 - dense_23_loss: 0.3392 - dense_24_loss: 0.4590 - dense_25_loss: 0.3392 - dense_14_f1: 0.6276 - dense_15_f1: 0.6074 - dense_16_f1: 0.5393 - dense_17_f1: 0.5618 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.5768 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3064 - val_dense_14_loss: 0.6591 - val_dense_15_loss: 0.6971 - val_dense_16_loss: 0.6929 - val_dense_17_loss: 0.6717 - val_dense_18_loss: 0.2778 - val_dense_19_loss: 0.3266 - val_dense_20_loss: 0.6446 - val_dense_21_loss: 0.6862 - val_dense_22_loss: 0.5497 - val_dense_23_loss: 0.3157 - val_dense_24_loss: 0.4147 - val_dense_25_loss: 0.3704 - val_dense_14_f1: 0.6477 - val_dense_15_f1: 0.4756 - val_dense_16_f1: 0.6200 - val_dense_17_f1: 0.6313 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.4875 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 14/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.3228 - dense_14_loss: 0.6564 - dense_15_loss: 0.6673 - dense_16_loss: 0.6827 - dense_17_loss: 0.6625 - dense_18_loss: 0.3816 - dense_19_loss: 0.3031 - dense_20_loss: 0.5972 - dense_21_loss: 0.6667 - dense_22_loss: 0.5896 - dense_23_loss: 0.3400 - dense_24_loss: 0.4544 - dense_25_loss: 0.3212 - dense_14_f1: 0.6487 - dense_15_f1: 0.5681 - dense_16_f1: 0.6203 - dense_17_f1: 0.6453 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6282 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3708 - val_dense_14_loss: 0.6648 - val_dense_15_loss: 0.6957 - val_dense_16_loss: 0.7131 - val_dense_17_loss: 0.6792 - val_dense_18_loss: 0.2910 - val_dense_19_loss: 0.3245 - val_dense_20_loss: 0.6419 - val_dense_21_loss: 0.6851 - val_dense_22_loss: 0.5637 - val_dense_23_loss: 0.3183 - val_dense_24_loss: 0.4196 - val_dense_25_loss: 0.3739 - val_dense_14_f1: 0.6501 - val_dense_15_f1: 0.5248 - val_dense_16_f1: 0.4405 - val_dense_17_f1: 0.5636 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6592 - val_dense_21_f1: 0.5626 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 15/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.2812 - dense_14_loss: 0.6485 - dense_15_loss: 0.6626 - dense_16_loss: 0.6846 - dense_17_loss: 0.6554 - dense_18_loss: 0.3812 - dense_19_loss: 0.3012 - dense_20_loss: 0.5950 - dense_21_loss: 0.6585 - dense_22_loss: 0.5834 - dense_23_loss: 0.3393 - dense_24_loss: 0.4544 - dense_25_loss: 0.3170 - dense_14_f1: 0.6352 - dense_15_f1: 0.6103 - dense_16_f1: 0.4756 - dense_17_f1: 0.6120 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6525 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3210 - val_dense_14_loss: 0.6509 - val_dense_15_loss: 0.6971 - val_dense_16_loss: 0.6995 - val_dense_17_loss: 0.6698 - val_dense_18_loss: 0.2953 - val_dense_19_loss: 0.3218 - val_dense_20_loss: 0.6532 - val_dense_21_loss: 0.6865 - val_dense_22_loss: 0.5429 - val_dense_23_loss: 0.3201 - val_dense_24_loss: 0.4140 - val_dense_25_loss: 0.3700 - val_dense_14_f1: 0.6411 - val_dense_15_f1: 0.5392 - val_dense_16_f1: 0.3052 - val_dense_17_f1: 0.5657 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5459 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 16/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.2596 - dense_14_loss: 0.6481 - dense_15_loss: 0.6596 - dense_16_loss: 0.6783 - dense_17_loss: 0.6529 - dense_18_loss: 0.3815 - dense_19_loss: 0.3008 - dense_20_loss: 0.5949 - dense_21_loss: 0.6542 - dense_22_loss: 0.5804 - dense_23_loss: 0.3385 - dense_24_loss: 0.4549 - dense_25_loss: 0.3155 - dense_14_f1: 0.6291 - dense_15_f1: 0.6427 - dense_16_f1: 0.4286 - dense_17_f1: 0.5875 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6329 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3001 - val_dense_14_loss: 0.6453 - val_dense_15_loss: 0.6988 - val_dense_16_loss: 0.6897 - val_dense_17_loss: 0.6679 - val_dense_18_loss: 0.2976 - val_dense_19_loss: 0.3214 - val_dense_20_loss: 0.6439 - val_dense_21_loss: 0.6833 - val_dense_22_loss: 0.5476 - val_dense_23_loss: 0.3151 - val_dense_24_loss: 0.4164 - val_dense_25_loss: 0.3731 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.6009 - val_dense_16_f1: 0.5462 - val_dense_17_f1: 0.5260 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5261 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 17/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.2264 - dense_14_loss: 0.6399 - dense_15_loss: 0.6597 - dense_16_loss: 0.6724 - dense_17_loss: 0.6524 - dense_18_loss: 0.3796 - dense_19_loss: 0.2990 - dense_20_loss: 0.5914 - dense_21_loss: 0.6510 - dense_22_loss: 0.5777 - dense_23_loss: 0.3374 - dense_24_loss: 0.4517 - dense_25_loss: 0.3142 - dense_14_f1: 0.6337 - dense_15_f1: 0.6561 - dense_16_f1: 0.6255 - dense_17_f1: 0.5921 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6348 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.2497 - val_dense_14_loss: 0.6402 - val_dense_15_loss: 0.6945 - val_dense_16_loss: 0.6872 - val_dense_17_loss: 0.6637 - val_dense_18_loss: 0.2837 - val_dense_19_loss: 0.3197 - val_dense_20_loss: 0.6442 - val_dense_21_loss: 0.6815 - val_dense_22_loss: 0.5389 - val_dense_23_loss: 0.3165 - val_dense_24_loss: 0.4097 - val_dense_25_loss: 0.3700 - val_dense_14_f1: 0.6475 - val_dense_15_f1: 0.5581 - val_dense_16_f1: 0.6027 - val_dense_17_f1: 0.5879 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6592 - val_dense_21_f1: 0.5223 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 18/30\n",
      "895/895 [==============================] - 9s 10ms/step - loss: 6.1855 - dense_14_loss: 0.6352 - dense_15_loss: 0.6547 - dense_16_loss: 0.6693 - dense_17_loss: 0.6468 - dense_18_loss: 0.3779 - dense_19_loss: 0.2963 - dense_20_loss: 0.5882 - dense_21_loss: 0.6468 - dense_22_loss: 0.5738 - dense_23_loss: 0.3371 - dense_24_loss: 0.4522 - dense_25_loss: 0.3074 - dense_14_f1: 0.6449 - dense_15_f1: 0.6207 - dense_16_f1: 0.6192 - dense_17_f1: 0.6444 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6994 - dense_21_f1: 0.6300 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.2174 - val_dense_14_loss: 0.6310 - val_dense_15_loss: 0.7012 - val_dense_16_loss: 0.6794 - val_dense_17_loss: 0.6741 - val_dense_18_loss: 0.2796 - val_dense_19_loss: 0.3158 - val_dense_20_loss: 0.6465 - val_dense_21_loss: 0.6795 - val_dense_22_loss: 0.5250 - val_dense_23_loss: 0.3190 - val_dense_24_loss: 0.4057 - val_dense_25_loss: 0.3605 - val_dense_14_f1: 0.6473 - val_dense_15_f1: 0.4890 - val_dense_16_f1: 0.5416 - val_dense_17_f1: 0.5744 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6592 - val_dense_21_f1: 0.5661 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 19/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.1667 - dense_14_loss: 0.6315 - dense_15_loss: 0.6519 - dense_16_loss: 0.6659 - dense_17_loss: 0.6465 - dense_18_loss: 0.3778 - dense_19_loss: 0.2949 - dense_20_loss: 0.5884 - dense_21_loss: 0.6484 - dense_22_loss: 0.5714 - dense_23_loss: 0.3354 - dense_24_loss: 0.4493 - dense_25_loss: 0.3053 - dense_14_f1: 0.6418 - dense_15_f1: 0.6056 - dense_16_f1: 0.6081 - dense_17_f1: 0.6198 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.7005 - dense_21_f1: 0.6291 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.2563 - val_dense_14_loss: 0.6391 - val_dense_15_loss: 0.7010 - val_dense_16_loss: 0.6836 - val_dense_17_loss: 0.6741 - val_dense_18_loss: 0.2856 - val_dense_19_loss: 0.3196 - val_dense_20_loss: 0.6405 - val_dense_21_loss: 0.6769 - val_dense_22_loss: 0.5415 - val_dense_23_loss: 0.3169 - val_dense_24_loss: 0.4161 - val_dense_25_loss: 0.3614 - val_dense_14_f1: 0.6413 - val_dense_15_f1: 0.5557 - val_dense_16_f1: 0.5327 - val_dense_17_f1: 0.5188 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6592 - val_dense_21_f1: 0.5487 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 20/30\n",
      "895/895 [==============================] - 8s 8ms/step - loss: 6.1512 - dense_14_loss: 0.6332 - dense_15_loss: 0.6529 - dense_16_loss: 0.6625 - dense_17_loss: 0.6429 - dense_18_loss: 0.3771 - dense_19_loss: 0.2944 - dense_20_loss: 0.5874 - dense_21_loss: 0.6438 - dense_22_loss: 0.5695 - dense_23_loss: 0.3358 - dense_24_loss: 0.4490 - dense_25_loss: 0.3028 - dense_14_f1: 0.6530 - dense_15_f1: 0.6145 - dense_16_f1: 0.6365 - dense_17_f1: 0.6119 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6997 - dense_21_f1: 0.6308 - dense_22_f1: 0.7162 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1832 - val_dense_14_loss: 0.6316 - val_dense_15_loss: 0.6981 - val_dense_16_loss: 0.6732 - val_dense_17_loss: 0.6605 - val_dense_18_loss: 0.2795 - val_dense_19_loss: 0.3152 - val_dense_20_loss: 0.6458 - val_dense_21_loss: 0.6742 - val_dense_22_loss: 0.5293 - val_dense_23_loss: 0.3174 - val_dense_24_loss: 0.4077 - val_dense_25_loss: 0.3506 - val_dense_14_f1: 0.6487 - val_dense_15_f1: 0.5324 - val_dense_16_f1: 0.6110 - val_dense_17_f1: 0.5852 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5505 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 21/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.1061 - dense_14_loss: 0.6249 - dense_15_loss: 0.6511 - dense_16_loss: 0.6550 - dense_17_loss: 0.6411 - dense_18_loss: 0.3732 - dense_19_loss: 0.2922 - dense_20_loss: 0.5844 - dense_21_loss: 0.6387 - dense_22_loss: 0.5660 - dense_23_loss: 0.3336 - dense_24_loss: 0.4497 - dense_25_loss: 0.2961 - dense_14_f1: 0.6565 - dense_15_f1: 0.6214 - dense_16_f1: 0.6522 - dense_17_f1: 0.6301 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6986 - dense_21_f1: 0.6599 - dense_22_f1: 0.7169 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1371 - val_dense_14_loss: 0.6205 - val_dense_15_loss: 0.6995 - val_dense_16_loss: 0.6613 - val_dense_17_loss: 0.6610 - val_dense_18_loss: 0.2731 - val_dense_19_loss: 0.3132 - val_dense_20_loss: 0.6534 - val_dense_21_loss: 0.6675 - val_dense_22_loss: 0.5203 - val_dense_23_loss: 0.3199 - val_dense_24_loss: 0.4061 - val_dense_25_loss: 0.3412 - val_dense_14_f1: 0.6424 - val_dense_15_f1: 0.5147 - val_dense_16_f1: 0.6096 - val_dense_17_f1: 0.6160 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.5947 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 22/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.1222 - dense_14_loss: 0.6198 - dense_15_loss: 0.6522 - dense_16_loss: 0.6538 - dense_17_loss: 0.6440 - dense_18_loss: 0.3767 - dense_19_loss: 0.2936 - dense_20_loss: 0.5912 - dense_21_loss: 0.6427 - dense_22_loss: 0.5669 - dense_23_loss: 0.3324 - dense_24_loss: 0.4519 - dense_25_loss: 0.2971 - dense_14_f1: 0.6607 - dense_15_f1: 0.5871 - dense_16_f1: 0.6459 - dense_17_f1: 0.6530 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6969 - dense_21_f1: 0.6441 - dense_22_f1: 0.7155 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1644 - val_dense_14_loss: 0.6106 - val_dense_15_loss: 0.7028 - val_dense_16_loss: 0.6697 - val_dense_17_loss: 0.6642 - val_dense_18_loss: 0.2840 - val_dense_19_loss: 0.3117 - val_dense_20_loss: 0.6419 - val_dense_21_loss: 0.6684 - val_dense_22_loss: 0.5227 - val_dense_23_loss: 0.3320 - val_dense_24_loss: 0.4084 - val_dense_25_loss: 0.3480 - val_dense_14_f1: 0.6856 - val_dense_15_f1: 0.4771 - val_dense_16_f1: 0.6386 - val_dense_17_f1: 0.5926 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6652 - val_dense_21_f1: 0.5760 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 23/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.0737 - dense_14_loss: 0.6106 - dense_15_loss: 0.6556 - dense_16_loss: 0.6484 - dense_17_loss: 0.6388 - dense_18_loss: 0.3734 - dense_19_loss: 0.2886 - dense_20_loss: 0.5845 - dense_21_loss: 0.6433 - dense_22_loss: 0.5603 - dense_23_loss: 0.3365 - dense_24_loss: 0.4454 - dense_25_loss: 0.2882 - dense_14_f1: 0.6949 - dense_15_f1: 0.5973 - dense_16_f1: 0.6339 - dense_17_f1: 0.6346 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.7065 - dense_21_f1: 0.6207 - dense_22_f1: 0.7187 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1998 - val_dense_14_loss: 0.6019 - val_dense_15_loss: 0.6994 - val_dense_16_loss: 0.6620 - val_dense_17_loss: 0.6726 - val_dense_18_loss: 0.3102 - val_dense_19_loss: 0.3170 - val_dense_20_loss: 0.6316 - val_dense_21_loss: 0.6753 - val_dense_22_loss: 0.5381 - val_dense_23_loss: 0.3237 - val_dense_24_loss: 0.4229 - val_dense_25_loss: 0.3452 - val_dense_14_f1: 0.6683 - val_dense_15_f1: 0.5666 - val_dense_16_f1: 0.6009 - val_dense_17_f1: 0.5570 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6696 - val_dense_21_f1: 0.5355 - val_dense_22_f1: 0.7736 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 24/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.0768 - dense_14_loss: 0.6111 - dense_15_loss: 0.6537 - dense_16_loss: 0.6417 - dense_17_loss: 0.6450 - dense_18_loss: 0.3762 - dense_19_loss: 0.2920 - dense_20_loss: 0.5899 - dense_21_loss: 0.6388 - dense_22_loss: 0.5603 - dense_23_loss: 0.3354 - dense_24_loss: 0.4516 - dense_25_loss: 0.2810 - dense_14_f1: 0.6737 - dense_15_f1: 0.6317 - dense_16_f1: 0.6406 - dense_17_f1: 0.6053 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6978 - dense_21_f1: 0.6305 - dense_22_f1: 0.7228 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1031 - val_dense_14_loss: 0.6046 - val_dense_15_loss: 0.7044 - val_dense_16_loss: 0.6446 - val_dense_17_loss: 0.6621 - val_dense_18_loss: 0.2745 - val_dense_19_loss: 0.3049 - val_dense_20_loss: 0.6660 - val_dense_21_loss: 0.6694 - val_dense_22_loss: 0.5117 - val_dense_23_loss: 0.3189 - val_dense_24_loss: 0.4099 - val_dense_25_loss: 0.3319 - val_dense_14_f1: 0.6506 - val_dense_15_f1: 0.5664 - val_dense_16_f1: 0.6394 - val_dense_17_f1: 0.5808 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6532 - val_dense_21_f1: 0.5805 - val_dense_22_f1: 0.7634 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 25/30\n",
      "895/895 [==============================] - 9s 10ms/step - loss: 6.0308 - dense_14_loss: 0.6128 - dense_15_loss: 0.6461 - dense_16_loss: 0.6417 - dense_17_loss: 0.6413 - dense_18_loss: 0.3681 - dense_19_loss: 0.2885 - dense_20_loss: 0.5866 - dense_21_loss: 0.6354 - dense_22_loss: 0.5552 - dense_23_loss: 0.3314 - dense_24_loss: 0.4438 - dense_25_loss: 0.2799 - dense_14_f1: 0.6500 - dense_15_f1: 0.6237 - dense_16_f1: 0.6320 - dense_17_f1: 0.6368 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6901 - dense_21_f1: 0.6276 - dense_22_f1: 0.7284 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.3172 - val_dense_14_loss: 0.6486 - val_dense_15_loss: 0.6983 - val_dense_16_loss: 0.6820 - val_dense_17_loss: 0.6848 - val_dense_18_loss: 0.2871 - val_dense_19_loss: 0.3205 - val_dense_20_loss: 0.6420 - val_dense_21_loss: 0.6709 - val_dense_22_loss: 0.5512 - val_dense_23_loss: 0.3396 - val_dense_24_loss: 0.4175 - val_dense_25_loss: 0.3748 - val_dense_14_f1: 0.5865 - val_dense_15_f1: 0.5482 - val_dense_16_f1: 0.5535 - val_dense_17_f1: 0.5541 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6262 - val_dense_21_f1: 0.5308 - val_dense_22_f1: 0.7682 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 26/30\n",
      "895/895 [==============================] - 10s 11ms/step - loss: 6.0463 - dense_14_loss: 0.6172 - dense_15_loss: 0.6420 - dense_16_loss: 0.6429 - dense_17_loss: 0.6439 - dense_18_loss: 0.3707 - dense_19_loss: 0.2878 - dense_20_loss: 0.5858 - dense_21_loss: 0.6299 - dense_22_loss: 0.5568 - dense_23_loss: 0.3318 - dense_24_loss: 0.4481 - dense_25_loss: 0.2894 - dense_14_f1: 0.6485 - dense_15_f1: 0.6278 - dense_16_f1: 0.6336 - dense_17_f1: 0.6200 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6882 - dense_21_f1: 0.6459 - dense_22_f1: 0.7249 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1047 - val_dense_14_loss: 0.6109 - val_dense_15_loss: 0.6991 - val_dense_16_loss: 0.6465 - val_dense_17_loss: 0.6607 - val_dense_18_loss: 0.2697 - val_dense_19_loss: 0.3176 - val_dense_20_loss: 0.6393 - val_dense_21_loss: 0.6691 - val_dense_22_loss: 0.5176 - val_dense_23_loss: 0.3264 - val_dense_24_loss: 0.4089 - val_dense_25_loss: 0.3391 - val_dense_14_f1: 0.6381 - val_dense_15_f1: 0.5577 - val_dense_16_f1: 0.6190 - val_dense_17_f1: 0.5937 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6516 - val_dense_21_f1: 0.5871 - val_dense_22_f1: 0.7589 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 27/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 5.9731 - dense_14_loss: 0.6081 - dense_15_loss: 0.6406 - dense_16_loss: 0.6328 - dense_17_loss: 0.6343 - dense_18_loss: 0.3661 - dense_19_loss: 0.2827 - dense_20_loss: 0.5795 - dense_21_loss: 0.6244 - dense_22_loss: 0.5534 - dense_23_loss: 0.3297 - dense_24_loss: 0.4442 - dense_25_loss: 0.2770 - dense_14_f1: 0.6691 - dense_15_f1: 0.6281 - dense_16_f1: 0.6635 - dense_17_f1: 0.6446 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6986 - dense_21_f1: 0.6581 - dense_22_f1: 0.7190 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8972 - val_loss: 6.1213 - val_dense_14_loss: 0.6146 - val_dense_15_loss: 0.7026 - val_dense_16_loss: 0.6626 - val_dense_17_loss: 0.6547 - val_dense_18_loss: 0.2737 - val_dense_19_loss: 0.3127 - val_dense_20_loss: 0.6454 - val_dense_21_loss: 0.6687 - val_dense_22_loss: 0.5197 - val_dense_23_loss: 0.3205 - val_dense_24_loss: 0.4081 - val_dense_25_loss: 0.3380 - val_dense_14_f1: 0.6524 - val_dense_15_f1: 0.5082 - val_dense_16_f1: 0.6188 - val_dense_17_f1: 0.6234 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6532 - val_dense_21_f1: 0.6008 - val_dense_22_f1: 0.7634 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 28/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 5.9582 - dense_14_loss: 0.6037 - dense_15_loss: 0.6396 - dense_16_loss: 0.6308 - dense_17_loss: 0.6330 - dense_18_loss: 0.3653 - dense_19_loss: 0.2843 - dense_20_loss: 0.5785 - dense_21_loss: 0.6237 - dense_22_loss: 0.5522 - dense_23_loss: 0.3306 - dense_24_loss: 0.4433 - dense_25_loss: 0.2730 - dense_14_f1: 0.6771 - dense_15_f1: 0.6187 - dense_16_f1: 0.6455 - dense_17_f1: 0.6585 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6984 - dense_21_f1: 0.6535 - dense_22_f1: 0.7278 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8966 - val_loss: 6.1162 - val_dense_14_loss: 0.6074 - val_dense_15_loss: 0.7045 - val_dense_16_loss: 0.6495 - val_dense_17_loss: 0.6612 - val_dense_18_loss: 0.2786 - val_dense_19_loss: 0.3138 - val_dense_20_loss: 0.6389 - val_dense_21_loss: 0.6647 - val_dense_22_loss: 0.5302 - val_dense_23_loss: 0.3201 - val_dense_24_loss: 0.4085 - val_dense_25_loss: 0.3388 - val_dense_14_f1: 0.6560 - val_dense_15_f1: 0.5409 - val_dense_16_f1: 0.6143 - val_dense_17_f1: 0.5934 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6517 - val_dense_21_f1: 0.5766 - val_dense_22_f1: 0.7623 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8770\n",
      "Epoch 29/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 5.9389 - dense_14_loss: 0.6002 - dense_15_loss: 0.6381 - dense_16_loss: 0.6298 - dense_17_loss: 0.6303 - dense_18_loss: 0.3593 - dense_19_loss: 0.2823 - dense_20_loss: 0.5798 - dense_21_loss: 0.6265 - dense_22_loss: 0.5521 - dense_23_loss: 0.3269 - dense_24_loss: 0.4411 - dense_25_loss: 0.2727 - dense_14_f1: 0.6731 - dense_15_f1: 0.6349 - dense_16_f1: 0.6339 - dense_17_f1: 0.6484 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.6973 - dense_21_f1: 0.6548 - dense_22_f1: 0.7219 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8992 - val_loss: 6.0688 - val_dense_14_loss: 0.5933 - val_dense_15_loss: 0.7020 - val_dense_16_loss: 0.6345 - val_dense_17_loss: 0.6651 - val_dense_18_loss: 0.2811 - val_dense_19_loss: 0.3139 - val_dense_20_loss: 0.6499 - val_dense_21_loss: 0.6575 - val_dense_22_loss: 0.5156 - val_dense_23_loss: 0.3148 - val_dense_24_loss: 0.4086 - val_dense_25_loss: 0.3323 - val_dense_14_f1: 0.6514 - val_dense_15_f1: 0.5078 - val_dense_16_f1: 0.6439 - val_dense_17_f1: 0.5986 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6562 - val_dense_21_f1: 0.6292 - val_dense_22_f1: 0.7651 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n",
      "Epoch 30/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 5.9227 - dense_14_loss: 0.5937 - dense_15_loss: 0.6390 - dense_16_loss: 0.6258 - dense_17_loss: 0.6322 - dense_18_loss: 0.3625 - dense_19_loss: 0.2831 - dense_20_loss: 0.5772 - dense_21_loss: 0.6305 - dense_22_loss: 0.5497 - dense_23_loss: 0.3250 - dense_24_loss: 0.4396 - dense_25_loss: 0.2644 - dense_14_f1: 0.6772 - dense_15_f1: 0.6449 - dense_16_f1: 0.6361 - dense_17_f1: 0.6461 - dense_18_f1: 0.8704 - dense_19_f1: 0.9084 - dense_20_f1: 0.7005 - dense_21_f1: 0.6510 - dense_22_f1: 0.7241 - dense_23_f1: 0.8894 - dense_24_f1: 0.8257 - dense_25_f1: 0.8971 - val_loss: 6.0980 - val_dense_14_loss: 0.5993 - val_dense_15_loss: 0.6992 - val_dense_16_loss: 0.6418 - val_dense_17_loss: 0.6703 - val_dense_18_loss: 0.2807 - val_dense_19_loss: 0.3133 - val_dense_20_loss: 0.6455 - val_dense_21_loss: 0.6598 - val_dense_22_loss: 0.5278 - val_dense_23_loss: 0.3172 - val_dense_24_loss: 0.4119 - val_dense_25_loss: 0.3313 - val_dense_14_f1: 0.6809 - val_dense_15_f1: 0.5343 - val_dense_16_f1: 0.6424 - val_dense_17_f1: 0.5934 - val_dense_18_f1: 0.9241 - val_dense_19_f1: 0.9018 - val_dense_20_f1: 0.6518 - val_dense_21_f1: 0.6104 - val_dense_22_f1: 0.7606 - val_dense_23_f1: 0.9062 - val_dense_24_f1: 0.8527 - val_dense_25_f1: 0.8750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y=[y1_train, y2_train, y3_train, y4_train, y5_train, y6_train, y7_train,y8_train,y9_train,y10_train,y11_train,y12_train], batch_size=128, epochs=30, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of anger   : 0.4893048128342246\n",
      "accuracy score of anticipation   : 0.4304812834224599\n",
      "accuracy score of disgust   : 0.49732620320855614\n",
      "accuracy score of fear   : 0.44919786096256686\n",
      "accuracy score of joy   : 0.3074866310160428\n",
      "accuracy score of love   : 0.4679144385026738\n",
      "accuracy score of optimism   : 0.39572192513368987\n",
      "accuracy score of pessimism   : 0.43315508021390375\n",
      "accuracy score of sadness   : 0.35294117647058826\n",
      "accuracy score of surprise   : 0.4679144385026738\n",
      "accuracy score of trust   : 0.446524064171123\n",
      "accuracy score of neutral   : 0.4037433155080214\n"
     ]
    }
   ],
   "source": [
    "emotion=[\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"]\n",
    "prediction = model.predict(X_test)\n",
    "import sklearn\n",
    "for j in range(12):\n",
    "    anger=[]\n",
    "    for i in range(len(prediction[1])):\n",
    "        if prediction[j][i][0]>prediction[j][i][1]:\n",
    "            anger.append(0)\n",
    "        else:\n",
    "            anger.append(1)\n",
    "    #print(prediction[0]) \n",
    "    \n",
    "    print(\"accuracy score of \"+emotion[j]+\"   : \"+str(sklearn.metrics.accuracy_score(test[emotion[j]],anger)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of anger   : 0.4589235127478753\n",
      "f1 score of anticipation   : 0.20817843866171\n",
      "f1 score of disgust   : 0.5982905982905983\n",
      "f1 score of fear   : 0.47715736040609136\n",
      "f1 score of joy   : 0.26210826210826216\n",
      "f1 score of love   : 0.17427385892116184\n",
      "f1 score of optimism   : 0.5291666666666666\n",
      "f1 score of pessimism   : 0.152\n",
      "f1 score of sadness   : 0.45739910313901344\n",
      "f1 score of surprise   : 0.18775510204081633\n",
      "f1 score of trust   : 0.23048327137546468\n",
      "f1 score of neutral   : 0.21754385964912284\n"
     ]
    }
   ],
   "source": [
    "emotion=[\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"]\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "for j in range(12):\n",
    "    anger=[]\n",
    "    for i in range(len(prediction[1])):\n",
    "        if prediction[j][i][0]>prediction[j][i][1]:\n",
    "            anger.append(0)\n",
    "        else:\n",
    "            anger.append(1)\n",
    "    #print(prediction[0]) \n",
    "    \n",
    "    print(\"f1 score of \"+emotion[j]+\"   : \"+str(sklearn.metrics.f1_score(test[emotion[j]],anger)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"neuralmodel.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
