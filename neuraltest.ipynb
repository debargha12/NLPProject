{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to the dataframe\n"
     ]
    }
   ],
   "source": [
    "f = open('nlp_train.json') \n",
    "import pandas as pd\n",
    "import json\n",
    "data = json.load(f) \n",
    "\n",
    "    # Iterating through the json \n",
    "    # list \n",
    "    #print(data[\"fkrr36o\"][\"emotion\"])\n",
    "f.close() \n",
    "fl=[]\n",
    "    #print(data[\"fkrr36o\"])\n",
    "for u,v in data.items():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    l.append(v[\"body\"])\n",
    "    l.append(1 if v[\"emotion\"][\"anger\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"anticipation\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"disgust\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"fear\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"joy\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"love\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"optimism\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"pessimism\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"sadness\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"surprise\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"trust\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"neutral\"]==True else 0)\n",
    "    fl.append(l)\n",
    "\n",
    "df = pd.DataFrame(fl, columns=[\"id\",\"body\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"])\n",
    "df.to_pickle(\"./dataframe.pkl\")\n",
    "print(\"Data loaded to the dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataframe.pkl\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    data = data.lower()\n",
    "    data = re.sub('<[^<]+?>', '', data)\n",
    "    data=re.sub('[!#?,.:\";]', '', data)\n",
    "    data=re.sub(r'[0-9]+', '', data)\n",
    "    data= re.sub(\"i'm\",\"i am\",data)\n",
    "    data = stemSentence(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing started\n",
      "Preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing started\")\n",
    "df['body'] = df['body'].map(lambda x : clean(x))\n",
    "print(\"Preprocessing completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 14)\n",
      "(1119, 12)\n",
      "(1119, 12)\n",
      "(1119, 12)\n",
      "(1119, 12)\n",
      "(374,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "emotions = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"]\n",
    "train, test= train_test_split(df, random_state=40, test_size=0.25, shuffle=True)\n",
    "\n",
    "X_train= train['body']\n",
    "X_test= test['body']\n",
    "y_train=train.iloc[:,2:]\n",
    "y_test=test.iloc[:,2:]\n",
    "print(train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(y1_train):\n",
    "    Y1_train=[]\n",
    "    \n",
    "    for i in y1_train:\n",
    "        if i==0:\n",
    "            Y1_train.append([1,0])\n",
    "        else:\n",
    "            Y1_train.append([0,1])\n",
    "    return Y1_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train = prep(train[\"anger\"].values)\n",
    "y1_test =  prep(test[\"anger\"].values)\n",
    "\n",
    "\n",
    "y2_train = prep(train[\"anticipation\"].values)\n",
    "y2_test =  prep(test[\"anticipation\"].values)\n",
    "\n",
    "y3_train = prep(train[\"disgust\"].values)\n",
    "y3_test =  prep(test[\"disgust\"].values)\n",
    "\n",
    "y4_train = prep(train[\"fear\"].values)\n",
    "y4_test =  prep(test[\"fear\"].values)\n",
    "\n",
    "y5_train = prep(train[\"joy\"].values)\n",
    "y5_test =  prep(test[[\"joy\"]].values)\n",
    "\n",
    "y6_train = prep(train[\"love\"].values)\n",
    "y6_test =  prep(train[\"love\"].values)\n",
    "\n",
    "y7_train = prep(train[\"optimism\"].values)\n",
    "y7_test =  prep(train[\"optimism\"].values)\n",
    "\n",
    "y8_train = prep(train[\"pessimism\"].values)\n",
    "y8_test =  prep(train[\"pessimism\"].values)\n",
    "\n",
    "y9_train = prep(train[\"sadness\"].values)\n",
    "y9_test =  prep(train[\"sadness\"].values)\n",
    "\n",
    "y10_train = prep(train[\"surprise\"].values)\n",
    "y10_test =  prep(train[\"surprise\"].values)\n",
    "\n",
    "y11_train = prep(train[\"trust\"].values)\n",
    "y11_test =  prep(train[\"trust\"].values)\n",
    "\n",
    "y12_train = prep(train[\"neutral\"].values)\n",
    "y12_test =  prep(train[\"neutral\"].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "embeddings_dictionary={}\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\n",
    "LSTM_Layer1 = LSTM(128)(embedding_layer)\n",
    "#LSTM_Layer2 = LSTM(128)(LSTM_Layer1)\n",
    "\n",
    "output1 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output2 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output3 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output4 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output5 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output6 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output7 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output8= Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output9 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output10 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output11 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "output12 = Dense(2, activation='sigmoid')(LSTM_Layer1)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6, output7, output8,output9, output10, output11, output12])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     1980600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          117248      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2)            258         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2)            258         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,100,944\n",
      "Trainable params: 120,344\n",
      "Non-trainable params: 1,980,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1119, 200)\n",
      "(1119, 12)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 895 samples, validate on 224 samples\n",
      "Epoch 1/30\n",
      "895/895 [==============================] - 12s 14ms/step - loss: 7.8523 - dense_1_loss: 0.6884 - dense_2_loss: 0.6945 - dense_3_loss: 0.6940 - dense_4_loss: 0.6943 - dense_5_loss: 0.6459 - dense_6_loss: 0.6019 - dense_7_loss: 0.6686 - dense_8_loss: 0.7016 - dense_9_loss: 0.6838 - dense_10_loss: 0.6071 - dense_11_loss: 0.5936 - dense_12_loss: 0.5784 - dense_1_acc: 0.5140 - dense_2_acc: 0.4793 - dense_3_acc: 0.5028 - dense_4_acc: 0.5665 - dense_5_acc: 0.7402 - dense_6_acc: 0.7866 - dense_7_acc: 0.6754 - dense_8_acc: 0.5151 - dense_9_acc: 0.6168 - dense_10_acc: 0.8106 - dense_11_acc: 0.8073 - dense_12_acc: 0.8508 - val_loss: 7.2410 - val_dense_1_loss: 0.6931 - val_dense_2_loss: 0.6911 - val_dense_3_loss: 0.6970 - val_dense_4_loss: 0.7009 - val_dense_5_loss: 0.5058 - val_dense_6_loss: 0.4717 - val_dense_7_loss: 0.6976 - val_dense_8_loss: 0.7092 - val_dense_9_loss: 0.6249 - val_dense_10_loss: 0.4685 - val_dense_11_loss: 0.5154 - val_dense_12_loss: 0.4658 - val_dense_1_acc: 0.4911 - val_dense_2_acc: 0.5424 - val_dense_3_acc: 0.5246 - val_dense_4_acc: 0.5424 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5134 - val_dense_9_acc: 0.7634 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 2/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 6.9680 - dense_1_loss: 0.6793 - dense_2_loss: 0.6983 - dense_3_loss: 0.6917 - dense_4_loss: 0.6885 - dense_5_loss: 0.4641 - dense_6_loss: 0.3946 - dense_7_loss: 0.6582 - dense_8_loss: 0.6989 - dense_9_loss: 0.6303 - dense_10_loss: 0.4485 - dense_11_loss: 0.5234 - dense_12_loss: 0.3920 - dense_1_acc: 0.5804 - dense_2_acc: 0.5067 - dense_3_acc: 0.5140 - dense_4_acc: 0.5687 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.5346 - dense_9_acc: 0.7156 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.4151 - val_dense_1_loss: 0.6588 - val_dense_2_loss: 0.7157 - val_dense_3_loss: 0.6931 - val_dense_4_loss: 0.6811 - val_dense_5_loss: 0.2929 - val_dense_6_loss: 0.3305 - val_dense_7_loss: 0.6650 - val_dense_8_loss: 0.6913 - val_dense_9_loss: 0.5585 - val_dense_10_loss: 0.3226 - val_dense_11_loss: 0.4279 - val_dense_12_loss: 0.3776 - val_dense_1_acc: 0.6473 - val_dense_2_acc: 0.5000 - val_dense_3_acc: 0.5022 - val_dense_4_acc: 0.5714 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5223 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 3/30\n",
      "895/895 [==============================] - 9s 10ms/step - loss: 6.4636 - dense_1_loss: 0.6572 - dense_2_loss: 0.6984 - dense_3_loss: 0.6875 - dense_4_loss: 0.6825 - dense_5_loss: 0.3875 - dense_6_loss: 0.3099 - dense_7_loss: 0.6147 - dense_8_loss: 0.6824 - dense_9_loss: 0.5951 - dense_10_loss: 0.3528 - dense_11_loss: 0.4671 - dense_12_loss: 0.3286 - dense_1_acc: 0.6268 - dense_2_acc: 0.5106 - dense_3_acc: 0.5436 - dense_4_acc: 0.5480 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.5709 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.3351 - val_dense_1_loss: 0.6507 - val_dense_2_loss: 0.6883 - val_dense_3_loss: 0.6895 - val_dense_4_loss: 0.6828 - val_dense_5_loss: 0.2736 - val_dense_6_loss: 0.3336 - val_dense_7_loss: 0.6486 - val_dense_8_loss: 0.6954 - val_dense_9_loss: 0.5538 - val_dense_10_loss: 0.3121 - val_dense_11_loss: 0.4226 - val_dense_12_loss: 0.3842 - val_dense_1_acc: 0.6473 - val_dense_2_acc: 0.5156 - val_dense_3_acc: 0.5268 - val_dense_4_acc: 0.5759 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5402 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 4/30\n",
      "895/895 [==============================] - 9s 10ms/step - loss: 6.4193 - dense_1_loss: 0.6565 - dense_2_loss: 0.6862 - dense_3_loss: 0.6828 - dense_4_loss: 0.6760 - dense_5_loss: 0.3894 - dense_6_loss: 0.3097 - dense_7_loss: 0.6097 - dense_8_loss: 0.6754 - dense_9_loss: 0.5907 - dense_10_loss: 0.3496 - dense_11_loss: 0.4650 - dense_12_loss: 0.3283 - dense_1_acc: 0.6268 - dense_2_acc: 0.5229 - dense_3_acc: 0.5497 - dense_4_acc: 0.5860 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.5715 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2996 - val_dense_1_loss: 0.6454 - val_dense_2_loss: 0.6865 - val_dense_3_loss: 0.6859 - val_dense_4_loss: 0.6748 - val_dense_5_loss: 0.2785 - val_dense_6_loss: 0.3256 - val_dense_7_loss: 0.6433 - val_dense_8_loss: 0.6869 - val_dense_9_loss: 0.5493 - val_dense_10_loss: 0.3119 - val_dense_11_loss: 0.4315 - val_dense_12_loss: 0.3802 - val_dense_1_acc: 0.6473 - val_dense_2_acc: 0.5379 - val_dense_3_acc: 0.5469 - val_dense_4_acc: 0.5714 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5290 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 5/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.3538 - dense_1_loss: 0.6492 - dense_2_loss: 0.6756 - dense_3_loss: 0.6758 - dense_4_loss: 0.6687 - dense_5_loss: 0.3874 - dense_6_loss: 0.3040 - dense_7_loss: 0.6028 - dense_8_loss: 0.6683 - dense_9_loss: 0.5855 - dense_10_loss: 0.3482 - dense_11_loss: 0.4626 - dense_12_loss: 0.3257 - dense_1_acc: 0.6268 - dense_2_acc: 0.6000 - dense_3_acc: 0.5860 - dense_4_acc: 0.5844 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6168 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2636 - val_dense_1_loss: 0.6388 - val_dense_2_loss: 0.6867 - val_dense_3_loss: 0.6798 - val_dense_4_loss: 0.6696 - val_dense_5_loss: 0.2901 - val_dense_6_loss: 0.3196 - val_dense_7_loss: 0.6443 - val_dense_8_loss: 0.6827 - val_dense_9_loss: 0.5440 - val_dense_10_loss: 0.3120 - val_dense_11_loss: 0.4214 - val_dense_12_loss: 0.3746 - val_dense_1_acc: 0.6473 - val_dense_2_acc: 0.5446 - val_dense_3_acc: 0.5558 - val_dense_4_acc: 0.5938 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5357 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 6/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.2984 - dense_1_loss: 0.6407 - dense_2_loss: 0.6708 - dense_3_loss: 0.6678 - dense_4_loss: 0.6586 - dense_5_loss: 0.3864 - dense_6_loss: 0.3028 - dense_7_loss: 0.6002 - dense_8_loss: 0.6616 - dense_9_loss: 0.5818 - dense_10_loss: 0.3463 - dense_11_loss: 0.4605 - dense_12_loss: 0.3209 - dense_1_acc: 0.6318 - dense_2_acc: 0.6022 - dense_3_acc: 0.6201 - dense_4_acc: 0.6145 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6173 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2240 - val_dense_1_loss: 0.6336 - val_dense_2_loss: 0.6882 - val_dense_3_loss: 0.6768 - val_dense_4_loss: 0.6674 - val_dense_5_loss: 0.2914 - val_dense_6_loss: 0.3155 - val_dense_7_loss: 0.6328 - val_dense_8_loss: 0.6725 - val_dense_9_loss: 0.5437 - val_dense_10_loss: 0.3166 - val_dense_11_loss: 0.4144 - val_dense_12_loss: 0.3713 - val_dense_1_acc: 0.6429 - val_dense_2_acc: 0.5513 - val_dense_3_acc: 0.5982 - val_dense_4_acc: 0.5960 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5714 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 7/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.2748 - dense_1_loss: 0.6436 - dense_2_loss: 0.6639 - dense_3_loss: 0.6679 - dense_4_loss: 0.6539 - dense_5_loss: 0.3862 - dense_6_loss: 0.3013 - dense_7_loss: 0.5970 - dense_8_loss: 0.6596 - dense_9_loss: 0.5810 - dense_10_loss: 0.3448 - dense_11_loss: 0.4579 - dense_12_loss: 0.3177 - dense_1_acc: 0.6374 - dense_2_acc: 0.6078 - dense_3_acc: 0.6006 - dense_4_acc: 0.6274 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6128 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2049 - val_dense_1_loss: 0.6291 - val_dense_2_loss: 0.6925 - val_dense_3_loss: 0.6680 - val_dense_4_loss: 0.6656 - val_dense_5_loss: 0.2907 - val_dense_6_loss: 0.3111 - val_dense_7_loss: 0.6445 - val_dense_8_loss: 0.6791 - val_dense_9_loss: 0.5280 - val_dense_10_loss: 0.3163 - val_dense_11_loss: 0.4137 - val_dense_12_loss: 0.3665 - val_dense_1_acc: 0.6473 - val_dense_2_acc: 0.5446 - val_dense_3_acc: 0.5714 - val_dense_4_acc: 0.5759 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5424 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 8/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.2039 - dense_1_loss: 0.6308 - dense_2_loss: 0.6612 - dense_3_loss: 0.6567 - dense_4_loss: 0.6470 - dense_5_loss: 0.3842 - dense_6_loss: 0.2979 - dense_7_loss: 0.5937 - dense_8_loss: 0.6523 - dense_9_loss: 0.5734 - dense_10_loss: 0.3421 - dense_11_loss: 0.4559 - dense_12_loss: 0.3086 - dense_1_acc: 0.6318 - dense_2_acc: 0.6061 - dense_3_acc: 0.6223 - dense_4_acc: 0.6196 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6251 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2132 - val_dense_1_loss: 0.6311 - val_dense_2_loss: 0.6867 - val_dense_3_loss: 0.6757 - val_dense_4_loss: 0.6633 - val_dense_5_loss: 0.2854 - val_dense_6_loss: 0.3113 - val_dense_7_loss: 0.6312 - val_dense_8_loss: 0.6708 - val_dense_9_loss: 0.5523 - val_dense_10_loss: 0.3195 - val_dense_11_loss: 0.4149 - val_dense_12_loss: 0.3708 - val_dense_1_acc: 0.6384 - val_dense_2_acc: 0.5804 - val_dense_3_acc: 0.5781 - val_dense_4_acc: 0.5625 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5826 - val_dense_9_acc: 0.7567 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 9/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.3135 - dense_1_loss: 0.6433 - dense_2_loss: 0.6740 - dense_3_loss: 0.6698 - dense_4_loss: 0.6692 - dense_5_loss: 0.3810 - dense_6_loss: 0.2958 - dense_7_loss: 0.6014 - dense_8_loss: 0.6774 - dense_9_loss: 0.5781 - dense_10_loss: 0.3438 - dense_11_loss: 0.4590 - dense_12_loss: 0.3207 - dense_1_acc: 0.6587 - dense_2_acc: 0.5682 - dense_3_acc: 0.5760 - dense_4_acc: 0.6000 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.5844 - dense_9_acc: 0.7168 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2146 - val_dense_1_loss: 0.6306 - val_dense_2_loss: 0.7018 - val_dense_3_loss: 0.6640 - val_dense_4_loss: 0.6761 - val_dense_5_loss: 0.2767 - val_dense_6_loss: 0.3087 - val_dense_7_loss: 0.6548 - val_dense_8_loss: 0.6878 - val_dense_9_loss: 0.5290 - val_dense_10_loss: 0.3126 - val_dense_11_loss: 0.4136 - val_dense_12_loss: 0.3588 - val_dense_1_acc: 0.6451 - val_dense_2_acc: 0.5112 - val_dense_3_acc: 0.6116 - val_dense_4_acc: 0.5692 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5603 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 10/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.1963 - dense_1_loss: 0.6300 - dense_2_loss: 0.6592 - dense_3_loss: 0.6502 - dense_4_loss: 0.6515 - dense_5_loss: 0.3759 - dense_6_loss: 0.2971 - dense_7_loss: 0.5945 - dense_8_loss: 0.6540 - dense_9_loss: 0.5808 - dense_10_loss: 0.3404 - dense_11_loss: 0.4571 - dense_12_loss: 0.3056 - dense_1_acc: 0.6598 - dense_2_acc: 0.6084 - dense_3_acc: 0.6223 - dense_4_acc: 0.6184 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6196 - dense_9_acc: 0.7168 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2611 - val_dense_1_loss: 0.6320 - val_dense_2_loss: 0.6917 - val_dense_3_loss: 0.6796 - val_dense_4_loss: 0.6965 - val_dense_5_loss: 0.2830 - val_dense_6_loss: 0.3101 - val_dense_7_loss: 0.6340 - val_dense_8_loss: 0.6737 - val_dense_9_loss: 0.5508 - val_dense_10_loss: 0.3173 - val_dense_11_loss: 0.4209 - val_dense_12_loss: 0.3716 - val_dense_1_acc: 0.6607 - val_dense_2_acc: 0.5536 - val_dense_3_acc: 0.5513 - val_dense_4_acc: 0.5402 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.6205 - val_dense_9_acc: 0.7567 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 11/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.1737 - dense_1_loss: 0.6176 - dense_2_loss: 0.6587 - dense_3_loss: 0.6474 - dense_4_loss: 0.6591 - dense_5_loss: 0.3751 - dense_6_loss: 0.2960 - dense_7_loss: 0.5925 - dense_8_loss: 0.6547 - dense_9_loss: 0.5732 - dense_10_loss: 0.3401 - dense_11_loss: 0.4566 - dense_12_loss: 0.3026 - dense_1_acc: 0.6682 - dense_2_acc: 0.6123 - dense_3_acc: 0.6089 - dense_4_acc: 0.6095 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6274 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.1192 - val_dense_1_loss: 0.6095 - val_dense_2_loss: 0.6814 - val_dense_3_loss: 0.6530 - val_dense_4_loss: 0.6696 - val_dense_5_loss: 0.2723 - val_dense_6_loss: 0.3064 - val_dense_7_loss: 0.6384 - val_dense_8_loss: 0.6668 - val_dense_9_loss: 0.5322 - val_dense_10_loss: 0.3148 - val_dense_11_loss: 0.4155 - val_dense_12_loss: 0.3593 - val_dense_1_acc: 0.6562 - val_dense_2_acc: 0.5603 - val_dense_3_acc: 0.6384 - val_dense_4_acc: 0.5692 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5714 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 12/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.1109 - dense_1_loss: 0.6074 - dense_2_loss: 0.6577 - dense_3_loss: 0.6365 - dense_4_loss: 0.6527 - dense_5_loss: 0.3716 - dense_6_loss: 0.2950 - dense_7_loss: 0.5911 - dense_8_loss: 0.6455 - dense_9_loss: 0.5675 - dense_10_loss: 0.3385 - dense_11_loss: 0.4534 - dense_12_loss: 0.2939 - dense_1_acc: 0.6788 - dense_2_acc: 0.6307 - dense_3_acc: 0.6363 - dense_4_acc: 0.6274 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6318 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0872 - val_dense_1_loss: 0.5960 - val_dense_2_loss: 0.6876 - val_dense_3_loss: 0.6354 - val_dense_4_loss: 0.6624 - val_dense_5_loss: 0.2786 - val_dense_6_loss: 0.3078 - val_dense_7_loss: 0.6330 - val_dense_8_loss: 0.6696 - val_dense_9_loss: 0.5292 - val_dense_10_loss: 0.3195 - val_dense_11_loss: 0.4185 - val_dense_12_loss: 0.3495 - val_dense_1_acc: 0.6763 - val_dense_2_acc: 0.5558 - val_dense_3_acc: 0.6317 - val_dense_4_acc: 0.5982 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6585 - val_dense_8_acc: 0.5491 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 13/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0638 - dense_1_loss: 0.6006 - dense_2_loss: 0.6565 - dense_3_loss: 0.6251 - dense_4_loss: 0.6458 - dense_5_loss: 0.3716 - dense_6_loss: 0.2935 - dense_7_loss: 0.5906 - dense_8_loss: 0.6435 - dense_9_loss: 0.5641 - dense_10_loss: 0.3382 - dense_11_loss: 0.4522 - dense_12_loss: 0.2821 - dense_1_acc: 0.6732 - dense_2_acc: 0.6179 - dense_3_acc: 0.6508 - dense_4_acc: 0.6358 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6983 - dense_8_acc: 0.6385 - dense_9_acc: 0.7168 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0504 - val_dense_1_loss: 0.5842 - val_dense_2_loss: 0.6922 - val_dense_3_loss: 0.6281 - val_dense_4_loss: 0.6658 - val_dense_5_loss: 0.2645 - val_dense_6_loss: 0.3083 - val_dense_7_loss: 0.6340 - val_dense_8_loss: 0.6645 - val_dense_9_loss: 0.5301 - val_dense_10_loss: 0.3172 - val_dense_11_loss: 0.4152 - val_dense_12_loss: 0.3463 - val_dense_1_acc: 0.6696 - val_dense_2_acc: 0.5491 - val_dense_3_acc: 0.6540 - val_dense_4_acc: 0.5759 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6607 - val_dense_8_acc: 0.5871 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 14/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0772 - dense_1_loss: 0.6109 - dense_2_loss: 0.6552 - dense_3_loss: 0.6381 - dense_4_loss: 0.6465 - dense_5_loss: 0.3722 - dense_6_loss: 0.2928 - dense_7_loss: 0.5886 - dense_8_loss: 0.6427 - dense_9_loss: 0.5660 - dense_10_loss: 0.3399 - dense_11_loss: 0.4500 - dense_12_loss: 0.2744 - dense_1_acc: 0.6430 - dense_2_acc: 0.6251 - dense_3_acc: 0.6391 - dense_4_acc: 0.6335 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6989 - dense_8_acc: 0.6413 - dense_9_acc: 0.7179 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0187 - val_dense_1_loss: 0.6052 - val_dense_2_loss: 0.6930 - val_dense_3_loss: 0.6232 - val_dense_4_loss: 0.6634 - val_dense_5_loss: 0.2659 - val_dense_6_loss: 0.3025 - val_dense_7_loss: 0.6401 - val_dense_8_loss: 0.6590 - val_dense_9_loss: 0.5153 - val_dense_10_loss: 0.3148 - val_dense_11_loss: 0.4071 - val_dense_12_loss: 0.3293 - val_dense_1_acc: 0.6451 - val_dense_2_acc: 0.5446 - val_dense_3_acc: 0.6362 - val_dense_4_acc: 0.5759 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5893 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 15/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0432 - dense_1_loss: 0.6040 - dense_2_loss: 0.6487 - dense_3_loss: 0.6297 - dense_4_loss: 0.6436 - dense_5_loss: 0.3695 - dense_6_loss: 0.2871 - dense_7_loss: 0.5880 - dense_8_loss: 0.6449 - dense_9_loss: 0.5600 - dense_10_loss: 0.3373 - dense_11_loss: 0.4514 - dense_12_loss: 0.2790 - dense_1_acc: 0.6760 - dense_2_acc: 0.6207 - dense_3_acc: 0.6464 - dense_4_acc: 0.6246 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7000 - dense_8_acc: 0.6419 - dense_9_acc: 0.7173 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2065 - val_dense_1_loss: 0.6245 - val_dense_2_loss: 0.6882 - val_dense_3_loss: 0.6703 - val_dense_4_loss: 0.6692 - val_dense_5_loss: 0.2912 - val_dense_6_loss: 0.3134 - val_dense_7_loss: 0.6220 - val_dense_8_loss: 0.6735 - val_dense_9_loss: 0.5461 - val_dense_10_loss: 0.3210 - val_dense_11_loss: 0.4112 - val_dense_12_loss: 0.3759 - val_dense_1_acc: 0.6205 - val_dense_2_acc: 0.5781 - val_dense_3_acc: 0.5759 - val_dense_4_acc: 0.5714 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5714 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 16/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.1005 - dense_1_loss: 0.6121 - dense_2_loss: 0.6463 - dense_3_loss: 0.6500 - dense_4_loss: 0.6458 - dense_5_loss: 0.3759 - dense_6_loss: 0.2883 - dense_7_loss: 0.5867 - dense_8_loss: 0.6533 - dense_9_loss: 0.5648 - dense_10_loss: 0.3365 - dense_11_loss: 0.4491 - dense_12_loss: 0.2917 - dense_1_acc: 0.6665 - dense_2_acc: 0.6313 - dense_3_acc: 0.6112 - dense_4_acc: 0.6134 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6983 - dense_8_acc: 0.6296 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0437 - val_dense_1_loss: 0.6007 - val_dense_2_loss: 0.6862 - val_dense_3_loss: 0.6323 - val_dense_4_loss: 0.6665 - val_dense_5_loss: 0.2686 - val_dense_6_loss: 0.3069 - val_dense_7_loss: 0.6340 - val_dense_8_loss: 0.6753 - val_dense_9_loss: 0.5200 - val_dense_10_loss: 0.3148 - val_dense_11_loss: 0.4114 - val_dense_12_loss: 0.3270 - val_dense_1_acc: 0.6585 - val_dense_2_acc: 0.5737 - val_dense_3_acc: 0.6071 - val_dense_4_acc: 0.5625 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5491 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 17/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0824 - dense_1_loss: 0.6175 - dense_2_loss: 0.6446 - dense_3_loss: 0.6384 - dense_4_loss: 0.6408 - dense_5_loss: 0.3735 - dense_6_loss: 0.2916 - dense_7_loss: 0.5883 - dense_8_loss: 0.6421 - dense_9_loss: 0.5644 - dense_10_loss: 0.3364 - dense_11_loss: 0.4488 - dense_12_loss: 0.2961 - dense_1_acc: 0.6615 - dense_2_acc: 0.6318 - dense_3_acc: 0.6285 - dense_4_acc: 0.6302 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6989 - dense_8_acc: 0.6346 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0355 - val_dense_1_loss: 0.5975 - val_dense_2_loss: 0.6890 - val_dense_3_loss: 0.6378 - val_dense_4_loss: 0.6568 - val_dense_5_loss: 0.2722 - val_dense_6_loss: 0.3103 - val_dense_7_loss: 0.6327 - val_dense_8_loss: 0.6650 - val_dense_9_loss: 0.5157 - val_dense_10_loss: 0.3169 - val_dense_11_loss: 0.4130 - val_dense_12_loss: 0.3284 - val_dense_1_acc: 0.6451 - val_dense_2_acc: 0.5513 - val_dense_3_acc: 0.6429 - val_dense_4_acc: 0.6071 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5759 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 18/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.1174 - dense_1_loss: 0.6150 - dense_2_loss: 0.6518 - dense_3_loss: 0.6413 - dense_4_loss: 0.6500 - dense_5_loss: 0.3785 - dense_6_loss: 0.2914 - dense_7_loss: 0.5887 - dense_8_loss: 0.6415 - dense_9_loss: 0.5697 - dense_10_loss: 0.3370 - dense_11_loss: 0.4507 - dense_12_loss: 0.3018 - dense_1_acc: 0.6665 - dense_2_acc: 0.6168 - dense_3_acc: 0.6207 - dense_4_acc: 0.6168 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6397 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0546 - val_dense_1_loss: 0.5997 - val_dense_2_loss: 0.6899 - val_dense_3_loss: 0.6464 - val_dense_4_loss: 0.6583 - val_dense_5_loss: 0.2688 - val_dense_6_loss: 0.3103 - val_dense_7_loss: 0.6312 - val_dense_8_loss: 0.6658 - val_dense_9_loss: 0.5245 - val_dense_10_loss: 0.3184 - val_dense_11_loss: 0.4110 - val_dense_12_loss: 0.3301 - val_dense_1_acc: 0.6406 - val_dense_2_acc: 0.5536 - val_dense_3_acc: 0.6317 - val_dense_4_acc: 0.6071 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6629 - val_dense_8_acc: 0.5871 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 19/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0533 - dense_1_loss: 0.6052 - dense_2_loss: 0.6478 - dense_3_loss: 0.6339 - dense_4_loss: 0.6409 - dense_5_loss: 0.3696 - dense_6_loss: 0.2922 - dense_7_loss: 0.5909 - dense_8_loss: 0.6438 - dense_9_loss: 0.5579 - dense_10_loss: 0.3353 - dense_11_loss: 0.4506 - dense_12_loss: 0.2852 - dense_1_acc: 0.6799 - dense_2_acc: 0.6279 - dense_3_acc: 0.6492 - dense_4_acc: 0.6307 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7006 - dense_8_acc: 0.6369 - dense_9_acc: 0.7207 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.1613 - val_dense_1_loss: 0.6195 - val_dense_2_loss: 0.7015 - val_dense_3_loss: 0.6642 - val_dense_4_loss: 0.6678 - val_dense_5_loss: 0.2702 - val_dense_6_loss: 0.3058 - val_dense_7_loss: 0.6377 - val_dense_8_loss: 0.6789 - val_dense_9_loss: 0.5343 - val_dense_10_loss: 0.3169 - val_dense_11_loss: 0.4128 - val_dense_12_loss: 0.3517 - val_dense_1_acc: 0.6696 - val_dense_2_acc: 0.5424 - val_dense_3_acc: 0.5871 - val_dense_4_acc: 0.5781 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5446 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 20/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0775 - dense_1_loss: 0.6082 - dense_2_loss: 0.6480 - dense_3_loss: 0.6401 - dense_4_loss: 0.6449 - dense_5_loss: 0.3655 - dense_6_loss: 0.2924 - dense_7_loss: 0.5917 - dense_8_loss: 0.6408 - dense_9_loss: 0.5651 - dense_10_loss: 0.3362 - dense_11_loss: 0.4553 - dense_12_loss: 0.2895 - dense_1_acc: 0.6592 - dense_2_acc: 0.6268 - dense_3_acc: 0.6212 - dense_4_acc: 0.6352 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6419 - dense_9_acc: 0.7162 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.1399 - val_dense_1_loss: 0.6018 - val_dense_2_loss: 0.6954 - val_dense_3_loss: 0.6599 - val_dense_4_loss: 0.6639 - val_dense_5_loss: 0.2795 - val_dense_6_loss: 0.3111 - val_dense_7_loss: 0.6466 - val_dense_8_loss: 0.6685 - val_dense_9_loss: 0.5366 - val_dense_10_loss: 0.3191 - val_dense_11_loss: 0.4149 - val_dense_12_loss: 0.3425 - val_dense_1_acc: 0.6629 - val_dense_2_acc: 0.5424 - val_dense_3_acc: 0.6116 - val_dense_4_acc: 0.5960 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5603 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 21/30\n",
      "895/895 [==============================] - 9s 10ms/step - loss: 6.0766 - dense_1_loss: 0.6172 - dense_2_loss: 0.6481 - dense_3_loss: 0.6429 - dense_4_loss: 0.6406 - dense_5_loss: 0.3712 - dense_6_loss: 0.2916 - dense_7_loss: 0.5946 - dense_8_loss: 0.6371 - dense_9_loss: 0.5607 - dense_10_loss: 0.3364 - dense_11_loss: 0.4498 - dense_12_loss: 0.2865 - dense_1_acc: 0.6637 - dense_2_acc: 0.6480 - dense_3_acc: 0.6145 - dense_4_acc: 0.6480 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.6994 - dense_8_acc: 0.6464 - dense_9_acc: 0.7218 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0884 - val_dense_1_loss: 0.5928 - val_dense_2_loss: 0.6929 - val_dense_3_loss: 0.6421 - val_dense_4_loss: 0.6667 - val_dense_5_loss: 0.2772 - val_dense_6_loss: 0.3033 - val_dense_7_loss: 0.6400 - val_dense_8_loss: 0.6665 - val_dense_9_loss: 0.5329 - val_dense_10_loss: 0.3224 - val_dense_11_loss: 0.4192 - val_dense_12_loss: 0.3324 - val_dense_1_acc: 0.6607 - val_dense_2_acc: 0.5558 - val_dense_3_acc: 0.6138 - val_dense_4_acc: 0.5804 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6562 - val_dense_8_acc: 0.5647 - val_dense_9_acc: 0.7545 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 22/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 5.9800 - dense_1_loss: 0.5931 - dense_2_loss: 0.6415 - dense_3_loss: 0.6202 - dense_4_loss: 0.6387 - dense_5_loss: 0.3604 - dense_6_loss: 0.2859 - dense_7_loss: 0.5891 - dense_8_loss: 0.6344 - dense_9_loss: 0.5557 - dense_10_loss: 0.3340 - dense_11_loss: 0.4485 - dense_12_loss: 0.2787 - dense_1_acc: 0.6838 - dense_2_acc: 0.6425 - dense_3_acc: 0.6413 - dense_4_acc: 0.6369 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7017 - dense_8_acc: 0.6503 - dense_9_acc: 0.7263 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8983 - val_loss: 6.1348 - val_dense_1_loss: 0.6075 - val_dense_2_loss: 0.6943 - val_dense_3_loss: 0.6504 - val_dense_4_loss: 0.6743 - val_dense_5_loss: 0.2823 - val_dense_6_loss: 0.2991 - val_dense_7_loss: 0.6322 - val_dense_8_loss: 0.6608 - val_dense_9_loss: 0.5395 - val_dense_10_loss: 0.3237 - val_dense_11_loss: 0.4176 - val_dense_12_loss: 0.3531 - val_dense_1_acc: 0.6652 - val_dense_2_acc: 0.5647 - val_dense_3_acc: 0.6362 - val_dense_4_acc: 0.5714 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6629 - val_dense_8_acc: 0.5871 - val_dense_9_acc: 0.7478 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 23/30\n",
      "895/895 [==============================] - 8s 8ms/step - loss: 5.9745 - dense_1_loss: 0.6028 - dense_2_loss: 0.6399 - dense_3_loss: 0.6229 - dense_4_loss: 0.6342 - dense_5_loss: 0.3685 - dense_6_loss: 0.2836 - dense_7_loss: 0.5832 - dense_8_loss: 0.6295 - dense_9_loss: 0.5517 - dense_10_loss: 0.3339 - dense_11_loss: 0.4444 - dense_12_loss: 0.2800 - dense_1_acc: 0.6721 - dense_2_acc: 0.6358 - dense_3_acc: 0.6503 - dense_4_acc: 0.6413 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7067 - dense_8_acc: 0.6514 - dense_9_acc: 0.7352 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8983 - val_loss: 6.1745 - val_dense_1_loss: 0.6145 - val_dense_2_loss: 0.6936 - val_dense_3_loss: 0.6610 - val_dense_4_loss: 0.6747 - val_dense_5_loss: 0.2904 - val_dense_6_loss: 0.3054 - val_dense_7_loss: 0.6294 - val_dense_8_loss: 0.6656 - val_dense_9_loss: 0.5349 - val_dense_10_loss: 0.3217 - val_dense_11_loss: 0.4133 - val_dense_12_loss: 0.3699 - val_dense_1_acc: 0.6652 - val_dense_2_acc: 0.5625 - val_dense_3_acc: 0.6384 - val_dense_4_acc: 0.5804 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6674 - val_dense_8_acc: 0.5692 - val_dense_9_acc: 0.7522 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 24/30\n",
      "895/895 [==============================] - 8s 8ms/step - loss: 5.9451 - dense_1_loss: 0.5908 - dense_2_loss: 0.6395 - dense_3_loss: 0.6233 - dense_4_loss: 0.6354 - dense_5_loss: 0.3685 - dense_6_loss: 0.2820 - dense_7_loss: 0.5804 - dense_8_loss: 0.6281 - dense_9_loss: 0.5495 - dense_10_loss: 0.3323 - dense_11_loss: 0.4399 - dense_12_loss: 0.2754 - dense_1_acc: 0.6922 - dense_2_acc: 0.6374 - dense_3_acc: 0.6503 - dense_4_acc: 0.6391 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7034 - dense_8_acc: 0.6553 - dense_9_acc: 0.7274 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8966 - val_loss: 6.0634 - val_dense_1_loss: 0.5965 - val_dense_2_loss: 0.6877 - val_dense_3_loss: 0.6404 - val_dense_4_loss: 0.6587 - val_dense_5_loss: 0.2739 - val_dense_6_loss: 0.3076 - val_dense_7_loss: 0.6319 - val_dense_8_loss: 0.6608 - val_dense_9_loss: 0.5276 - val_dense_10_loss: 0.3163 - val_dense_11_loss: 0.4110 - val_dense_12_loss: 0.3511 - val_dense_1_acc: 0.6808 - val_dense_2_acc: 0.5692 - val_dense_3_acc: 0.6429 - val_dense_4_acc: 0.5826 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6652 - val_dense_8_acc: 0.5647 - val_dense_9_acc: 0.7545 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8728\n",
      "Epoch 25/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 5.9734 - dense_1_loss: 0.6037 - dense_2_loss: 0.6336 - dense_3_loss: 0.6258 - dense_4_loss: 0.6287 - dense_5_loss: 0.3694 - dense_6_loss: 0.2875 - dense_7_loss: 0.5803 - dense_8_loss: 0.6310 - dense_9_loss: 0.5503 - dense_10_loss: 0.3318 - dense_11_loss: 0.4426 - dense_12_loss: 0.2888 - dense_1_acc: 0.6737 - dense_2_acc: 0.6447 - dense_3_acc: 0.6430 - dense_4_acc: 0.6447 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7034 - dense_8_acc: 0.6486 - dense_9_acc: 0.7307 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8961 - val_loss: 6.0639 - val_dense_1_loss: 0.6054 - val_dense_2_loss: 0.6900 - val_dense_3_loss: 0.6487 - val_dense_4_loss: 0.6578 - val_dense_5_loss: 0.2746 - val_dense_6_loss: 0.3103 - val_dense_7_loss: 0.6209 - val_dense_8_loss: 0.6651 - val_dense_9_loss: 0.5229 - val_dense_10_loss: 0.3160 - val_dense_11_loss: 0.4100 - val_dense_12_loss: 0.3424 - val_dense_1_acc: 0.6451 - val_dense_2_acc: 0.5692 - val_dense_3_acc: 0.5915 - val_dense_4_acc: 0.5938 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6719 - val_dense_8_acc: 0.5513 - val_dense_9_acc: 0.7567 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 26/30\n",
      "895/895 [==============================] - 8s 9ms/step - loss: 5.9768 - dense_1_loss: 0.6061 - dense_2_loss: 0.6334 - dense_3_loss: 0.6364 - dense_4_loss: 0.6343 - dense_5_loss: 0.3687 - dense_6_loss: 0.2849 - dense_7_loss: 0.5755 - dense_8_loss: 0.6307 - dense_9_loss: 0.5479 - dense_10_loss: 0.3325 - dense_11_loss: 0.4430 - dense_12_loss: 0.2834 - dense_1_acc: 0.6598 - dense_2_acc: 0.6453 - dense_3_acc: 0.6257 - dense_4_acc: 0.6374 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7034 - dense_8_acc: 0.6620 - dense_9_acc: 0.7179 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2716 - val_dense_1_loss: 0.6415 - val_dense_2_loss: 0.6932 - val_dense_3_loss: 0.7040 - val_dense_4_loss: 0.6652 - val_dense_5_loss: 0.2875 - val_dense_6_loss: 0.3180 - val_dense_7_loss: 0.6245 - val_dense_8_loss: 0.6675 - val_dense_9_loss: 0.5533 - val_dense_10_loss: 0.3252 - val_dense_11_loss: 0.4238 - val_dense_12_loss: 0.3677 - val_dense_1_acc: 0.6496 - val_dense_2_acc: 0.5513 - val_dense_3_acc: 0.5513 - val_dense_4_acc: 0.5714 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6607 - val_dense_8_acc: 0.6116 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 27/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 6.0430 - dense_1_loss: 0.6079 - dense_2_loss: 0.6357 - dense_3_loss: 0.6365 - dense_4_loss: 0.6337 - dense_5_loss: 0.3714 - dense_6_loss: 0.2914 - dense_7_loss: 0.5840 - dense_8_loss: 0.6391 - dense_9_loss: 0.5574 - dense_10_loss: 0.3343 - dense_11_loss: 0.4491 - dense_12_loss: 0.3024 - dense_1_acc: 0.6911 - dense_2_acc: 0.6413 - dense_3_acc: 0.6201 - dense_4_acc: 0.6609 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7034 - dense_8_acc: 0.6358 - dense_9_acc: 0.7229 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.2193 - val_dense_1_loss: 0.6281 - val_dense_2_loss: 0.7046 - val_dense_3_loss: 0.6717 - val_dense_4_loss: 0.6679 - val_dense_5_loss: 0.2834 - val_dense_6_loss: 0.3172 - val_dense_7_loss: 0.6247 - val_dense_8_loss: 0.6715 - val_dense_9_loss: 0.5418 - val_dense_10_loss: 0.3232 - val_dense_11_loss: 0.4231 - val_dense_12_loss: 0.3623 - val_dense_1_acc: 0.6429 - val_dense_2_acc: 0.5424 - val_dense_3_acc: 0.5714 - val_dense_4_acc: 0.5826 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6696 - val_dense_8_acc: 0.5379 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8750\n",
      "Epoch 28/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 5.9907 - dense_1_loss: 0.6046 - dense_2_loss: 0.6343 - dense_3_loss: 0.6314 - dense_4_loss: 0.6301 - dense_5_loss: 0.3705 - dense_6_loss: 0.2878 - dense_7_loss: 0.5791 - dense_8_loss: 0.6346 - dense_9_loss: 0.5523 - dense_10_loss: 0.3325 - dense_11_loss: 0.4436 - dense_12_loss: 0.2901 - dense_1_acc: 0.6704 - dense_2_acc: 0.6302 - dense_3_acc: 0.6447 - dense_4_acc: 0.6592 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7034 - dense_8_acc: 0.6492 - dense_9_acc: 0.7173 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.1440 - val_dense_1_loss: 0.6213 - val_dense_2_loss: 0.7001 - val_dense_3_loss: 0.6655 - val_dense_4_loss: 0.6600 - val_dense_5_loss: 0.2723 - val_dense_6_loss: 0.3111 - val_dense_7_loss: 0.6287 - val_dense_8_loss: 0.6799 - val_dense_9_loss: 0.5265 - val_dense_10_loss: 0.3220 - val_dense_11_loss: 0.4120 - val_dense_12_loss: 0.3447 - val_dense_1_acc: 0.6451 - val_dense_2_acc: 0.5357 - val_dense_3_acc: 0.5647 - val_dense_4_acc: 0.5893 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6607 - val_dense_8_acc: 0.5513 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8772\n",
      "Epoch 29/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 5.9513 - dense_1_loss: 0.5963 - dense_2_loss: 0.6438 - dense_3_loss: 0.6193 - dense_4_loss: 0.6347 - dense_5_loss: 0.3675 - dense_6_loss: 0.2859 - dense_7_loss: 0.5763 - dense_8_loss: 0.6257 - dense_9_loss: 0.5495 - dense_10_loss: 0.3307 - dense_11_loss: 0.4412 - dense_12_loss: 0.2804 - dense_1_acc: 0.6989 - dense_2_acc: 0.6475 - dense_3_acc: 0.6598 - dense_4_acc: 0.6408 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7017 - dense_8_acc: 0.6760 - dense_9_acc: 0.7168 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0843 - val_dense_1_loss: 0.6120 - val_dense_2_loss: 0.6969 - val_dense_3_loss: 0.6546 - val_dense_4_loss: 0.6557 - val_dense_5_loss: 0.2727 - val_dense_6_loss: 0.3140 - val_dense_7_loss: 0.6285 - val_dense_8_loss: 0.6610 - val_dense_9_loss: 0.5214 - val_dense_10_loss: 0.3195 - val_dense_11_loss: 0.4114 - val_dense_12_loss: 0.3366 - val_dense_1_acc: 0.6607 - val_dense_2_acc: 0.5446 - val_dense_3_acc: 0.5982 - val_dense_4_acc: 0.5893 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6674 - val_dense_8_acc: 0.5848 - val_dense_9_acc: 0.7589 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8817\n",
      "Epoch 30/30\n",
      "895/895 [==============================] - 7s 8ms/step - loss: 5.8862 - dense_1_loss: 0.5895 - dense_2_loss: 0.6290 - dense_3_loss: 0.6111 - dense_4_loss: 0.6251 - dense_5_loss: 0.3643 - dense_6_loss: 0.2849 - dense_7_loss: 0.5777 - dense_8_loss: 0.6178 - dense_9_loss: 0.5433 - dense_10_loss: 0.3310 - dense_11_loss: 0.4397 - dense_12_loss: 0.2726 - dense_1_acc: 0.6860 - dense_2_acc: 0.6480 - dense_3_acc: 0.6693 - dense_4_acc: 0.6564 - dense_5_acc: 0.8704 - dense_6_acc: 0.9084 - dense_7_acc: 0.7145 - dense_8_acc: 0.6832 - dense_9_acc: 0.7285 - dense_10_acc: 0.8894 - dense_11_acc: 0.8257 - dense_12_acc: 0.8972 - val_loss: 6.0801 - val_dense_1_loss: 0.6002 - val_dense_2_loss: 0.7041 - val_dense_3_loss: 0.6419 - val_dense_4_loss: 0.6697 - val_dense_5_loss: 0.2748 - val_dense_6_loss: 0.3119 - val_dense_7_loss: 0.6184 - val_dense_8_loss: 0.6592 - val_dense_9_loss: 0.5230 - val_dense_10_loss: 0.3215 - val_dense_11_loss: 0.4155 - val_dense_12_loss: 0.3400 - val_dense_1_acc: 0.6562 - val_dense_2_acc: 0.5759 - val_dense_3_acc: 0.6339 - val_dense_4_acc: 0.5647 - val_dense_5_acc: 0.9241 - val_dense_6_acc: 0.9018 - val_dense_7_acc: 0.6496 - val_dense_8_acc: 0.5915 - val_dense_9_acc: 0.7522 - val_dense_10_acc: 0.9062 - val_dense_11_acc: 0.8527 - val_dense_12_acc: 0.8817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y=[y1_train, y2_train, y3_train, y4_train, y5_train, y6_train, y7_train,y8_train,y9_train,y10_train,y11_train,y12_train], batch_size=128, epochs=30, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of anger   : 0.6497326203208557\n",
      "accuracy score of anticipation   : 0.6203208556149733\n",
      "accuracy score of disgust   : 0.6336898395721925\n",
      "accuracy score of fear   : 0.6096256684491979\n",
      "accuracy score of joy   : 0.8609625668449198\n",
      "accuracy score of love   : 0.9090909090909091\n",
      "accuracy score of optimism   : 0.6016042780748663\n",
      "accuracy score of pessimism   : 0.6524064171122995\n",
      "accuracy score of sadness   : 0.7058823529411765\n",
      "accuracy score of surprise   : 0.8877005347593583\n",
      "accuracy score of trust   : 0.8342245989304813\n",
      "accuracy score of neutral   : 0.8796791443850267\n"
     ]
    }
   ],
   "source": [
    "emotion=[\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"]\n",
    "prediction = model.predict(X_test)\n",
    "import sklearn\n",
    "for j in range(12):\n",
    "    anger=[]\n",
    "    for i in range(len(prediction[1])):\n",
    "        if prediction[j][i][0]>prediction[j][i][1]:\n",
    "            anger.append(0)\n",
    "        else:\n",
    "            anger.append(1)\n",
    "    #print(prediction[0]) \n",
    "    \n",
    "    print(\"accuracy score of \"+emotion[j]+\"   : \"+str(sklearn.metrics.accuracy_score(test[emotion[j]],anger)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of anger   : 0.2598870056497175\n",
      "f1 score of anticipation   : 0.6743119266055045\n",
      "f1 score of disgust   : 0.573208722741433\n",
      "f1 score of fear   : 0.635\n",
      "f1 score of joy   : 0.0\n",
      "f1 score of love   : 0.0\n",
      "f1 score of optimism   : 0.08588957055214724\n",
      "f1 score of pessimism   : 0.563758389261745\n",
      "f1 score of sadness   : 0.03508771929824561\n",
      "f1 score of surprise   : 0.0\n",
      "f1 score of trust   : 0.0\n",
      "f1 score of neutral   : 0.18181818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debargha\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "emotion=[\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"]\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "for j in range(12):\n",
    "    anger=[]\n",
    "    for i in range(len(prediction[1])):\n",
    "        if prediction[j][i][0]>prediction[j][i][1]:\n",
    "            anger.append(0)\n",
    "        else:\n",
    "            anger.append(1)\n",
    "    #print(prediction[0]) \n",
    "    \n",
    "    print(\"f1 score of \"+emotion[j]+\"   : \"+str(sklearn.metrics.f1_score(test[emotion[j]],anger)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"neuralmodel.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on nlp_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to the dataframe\n"
     ]
    }
   ],
   "source": [
    "f = open('nlp_test.json') \n",
    "import pandas as pd\n",
    "import json\n",
    "data = json.load(f) \n",
    "\n",
    "    # Iterating through the json \n",
    "    # list \n",
    "    #print(data[\"fkrr36o\"][\"emotion\"])\n",
    "f.close() \n",
    "fl=[]\n",
    "    #print(data[\"fkrr36o\"])\n",
    "for u,v in data.items():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    l.append(v[\"body\"])\n",
    "    l.append(1 if v[\"emotion\"][\"anger\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"anticipation\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"disgust\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"fear\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"joy\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"love\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"optimism\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"pessimism\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"sadness\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"surprise\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"trust\"]==True else 0)\n",
    "    l.append(1 if v[\"emotion\"][\"neutral\"]==True else 0)\n",
    "    fl.append(l)\n",
    "\n",
    "df = pd.DataFrame(fl, columns=[\"id\",\"body\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\",\"neutral\"])\n",
    "df.to_pickle(\"./dataframetest.pkl\")\n",
    "print(\"Data loaded to the dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataframetest.pkl\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    data = data.lower()\n",
    "    data = re.sub('<[^<]+?>', '', data)\n",
    "    data=re.sub('[!#?,.:\";]', '', data)\n",
    "    data=re.sub(r'[0-9]+', '', data)\n",
    "    data= re.sub(\"i'm\",\"i am\",data)\n",
    "    data = stemSentence(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing started\n",
      "Preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing started\")\n",
    "df['body'] = df['body'].map(lambda x : clean(x))\n",
    "print(\"Preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(y1_train):\n",
    "    Y1_train=[]\n",
    "    \n",
    "    for i in y1_train:\n",
    "        if i==0:\n",
    "            Y1_train.append([1,0])\n",
    "        else:\n",
    "            Y1_train.append([0,1])\n",
    "    return Y1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=df[\"body\"]\n",
    "y_test=df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "#X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "embeddings_dictionary={}\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374 374\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "f1 score of anger   : 0.0625\n",
      "f1 score of anticipation   : 0.6140724946695095\n",
      "f1 score of disgust   : 0.3666666666666667\n",
      "f1 score of fear   : 0.5898617511520737\n",
      "f1 score of joy   : 0.0\n",
      "f1 score of love   : 0.0\n",
      "f1 score of optimism   : 0.10975609756097562\n",
      "f1 score of pessimism   : 0.43333333333333335\n",
      "f1 score of sadness   : 0.0\n",
      "f1 score of surprise   : 0.0\n",
      "f1 score of trust   : 0.0\n",
      "f1 score of neutral   : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debargha\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = model.predict(X_test)\n",
    "print(len(prediction))\n",
    "for j in range(12):\n",
    "    anger=[]\n",
    "    for i in range(len(prediction[1])):\n",
    "        if prediction[j][i][0]>prediction[j][i][1]:\n",
    "            anger.append(0)\n",
    "        else:\n",
    "            anger.append(1)\n",
    "    #print(prediction[0]) \n",
    "    \n",
    "    print(\"f1 score of \"+emotion[j]+\"   : \"+str(sklearn.metrics.f1_score(test[emotion[j]],anger)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
